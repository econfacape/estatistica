---
title: "Introdução à Regressão Linear"
author: "João Ricardo F. de Lima"
date: "today"
editor: source
lang: pt
language: 
  toc-title-document: '<a href="https://www.facape.br/" target="_blank"><img src="https://github.com/econfacape/macroeconometria/blob/main/logofacape.jpg?raw=true" alt="Logotipo Facape" width="150"></a>'
format: 
  html:
    toc: true
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    toc-location: left
    code-fold: false
    embed-resources: true
    page-layout: full
    fig-asp: 0.618
    fig-width: 8
    fig-height: 5
    fig-dpi: 300
    fig-align: center
    df-print: paged
    fontsize: 13pt
theme:
  light: flatly
execute:
  echo: TRUE
  message: false
  warning: false
---

<br>

# A natureza da análise da regressão

Regressão Linear é 'a principal ferramenta da econometria é a ferramenta estatística que trata do estudo da dependência de uma variável, regressando (Y), em relação a uma ou mais variáveis explicativas (regressores), com o objetivo de estimar e/ou prever o valor médio de Y dados os valores de X. 

Apesar da análise de regressão tratar da dependência de uma variável sobre outras variáveis, isto não necessariamente implica "causa". Uma relação estatística em si não pode logicamente implicar em causa. Para atribuir causalidade deve-se considerar a teoria econômica ou mesmo o senso comum.

<br>

# Regressão versus Correlação

Relacionado mas conceitualmente muito diferente é a análise da regressão e de correlação. Na regressão é estimar ou prever o valor médio do regressando (Y) com base nos valores fixos dos regressores (X). Na correlação o objetivo é mensurar o grau de associação linear entre duas variáveis. 

As estimativas de correlação podem ser positivas ou negativas. O primeiro caso é quando uma variável cresce, a outra aumenta também. O segundo caso é o inverso.

O coeficiente de correlação de Pearson ($r$) pode ser encontrado pela fórmula:

$$r=\frac{\sum X_iY_i-\frac{\sum X_i-\sum Y_i}{n}}{\Bigg[\sum X_i^2-\frac{(\sum X_i)^2}{n}\Bigg]\Bigg[\sum Y_i^2-\frac{(\sum Y_i)^2}{n}\Bigg]}$$
A ocorrência de um valor de r=0 ou próximo de zero indica que não há correlação linear entre as variáveis, mas pode ocorrer não linear.

<br>

As principais características do coeficiente de correlação de Pearson são:

A) Seus valores estão compreendidos entre -1 e 1;

B) Se o coeficiente for positivo, as duas características estudadas tendem a variar no mesmo sentido;

C) Se o sinal for negativo, as duas características estudadas tendem a variar no sentido contrário;

D) A relação entre duas variáveis é tanto mais estreita quanto mais o coeficiente se aproxima de 1 ou de -1;

E) o valor de r é uma estimativa do parâmetro $\rho$ (rho) da mesma forma que a média ($\bar{x}$) é uma estimativa de $\mu$; 


# Modelo de Regressão Simples

## Definição do Modelo de Regressão Simples

O modelo de regressão simples pode ser usado para estudar a relação entre duas variáveis. Em outras palavras, quando se pretender analisar como "y" varia quando "x" mudar;

Considere duas variáveis "y" e "x" que representam alguma população. A equação abaixo relaciona as duas variáveis:

$$
	y=\beta_0+\beta_1x+u
$$

A equação acima é válida para a população de interesse e chamada de **modelo de regressão linear simples**. Pode ser também chamada de regressão bivariada ou de duas variáveis.

Quando relacionadas como acima, as variáveis *x* e *y* podem ser denominadas de diversas formas:

``` {r, include=FALSE}
#https://tableconvert.com/latex-to-markdown
```

<center>Tabela 1: Terminologia para a regressão simples: </center>


| Y                    | X                     |
|:--------------------:|:---------------------:|
| Variável Dependente  | Variável Independente |
| Variável Explicada   | Variável Explicativa  |
| Variável de Resposta | Variável de Controle  |
| Variável Prevista    | Variável Previsora    |
| Regressando          | Regressor             |
  

O termo covariável também pode ser usado para x. O mais comum é variável dependente e independente.

A variável *u*, chamada de termo de erro ou resíduo da regressão representa outros fatores, além de x, que afetam y, mas que não sao observados. Se os outros fatores forem mantidos fixos, sua variação é zero, ou seja, $\Delta u=0$. Assim, x tem um efeito linear sobre y:

$$
\Delta y=\beta_1 \Delta x \quad se \quad \Delta u=0
$$

A variação em y é, simplesmente, $\beta_1$ multiplicado pela variação em x. Isso significa que $\beta_1$ é o **parâmetro de inclinação** da relação entre y e x, mantendo fixos os fatores em u;


O **parâmetro de intercepto** $\beta_0$ é chamado de constante do modelo. 

O coeficiente de inclinação mede a taxa (parcial) da mudança no valor médio de y para uma mudança de uma unidade no valor da variável independente, tudo o mais constante; 

O termo de erro **u** representa todas aquelas variáveis que não podem estar explícitas no modelo por razões diversas. Contudo, se assume que a influência destas sobre o regressando é insignificante;

Pode-se entender o termo de erro como os fatores não observados que afetam y. 


## Derivação das estimativas de Mínimos Quadrados Ordinários (MQO)

Para estimar o Modelo de Regressão Linear, ou seja, encontrar os parâmetros $\beta_0$ e $\beta_1$ da primeira equação acima o método mais usado é o dos Mínimos Quadrados Ordinários (MQO), pois é intuitivamente e matematicamente mais simples; 

Dado que um Modelo de Regressão:

$$
Y=\widehat{\beta}_0+\widehat{\beta}_1X+\widehat{u}
$$

pode ser definido também como:

$$
\widehat{Y}=\widehat{\beta}_0+\widehat{\beta}_1X
$$

em que o "chápeu" sobre Y, os betas e u enfatiza que os valores são estimativas.

Pode-se reescrever de forma que: 

$$
Y=\widehat{Y}+\widehat{u}
$$

ou ainda:  

$$
\widehat{u}=Y-\widehat{Y}
$$  

então, um método bom para se obter as estimativas dos $\beta$s seria fazer com que a soma de todos os erros fosse a menor possível.

Por questões teóricas, o método dos Mínimos Quadrados Ordinários (MQO) minimiza a soma dos erros ao quadrado, dando peso diferente às observações cujo valor estimado esteja mais próximo ou mais distante do valor observado ($Y$); 

Assim, o que se precisa é minimizar uma função conhecida como Soma de Quadrados dos Resíduos (SQR): 

$$
min \sum\widehat{u}^2=\sum(Y-\widehat{\beta}_0-\widehat{\beta}_1X)^2
$$


Após a derivação pode-se mostrar que:

$$
\hat{\beta_0}=\overline{Y}-\hat{\beta_1}\overline{X}
$$

Uma vez que se tenha $\hat{\beta_1}$, dados os valores das médias de Y e X se obtém o valor de $\hat{\beta_0}$.

E ainda pode-se mostrar que:

$$
\hat{\beta_1}=\frac{\sum(X-\overline{X})(Y-\overline{Y})}{\sum(X-\overline{X})^2}
$$

$$
\hat{\beta_1}=\frac{\sum xy}{\sum x^2}
$$

com $x=X-\overline{X}$ e $y=Y-\overline{Y}$.

Uma vez determinados os estimadores de intercepto e inclinação de MQO (Mínimos Quadrados Ordinários), se constrói a **reta de regressão de MQO**:

$$
\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x
$$

A função dada em acima é chamada de função de regressão amostral (FRA), pois ela é uma versão estimada da função de regressão populacional $E(y|x)=\beta_{0}+\beta_1x$. 

Como a FRA é obtida para determinada amostra de dados, uma amostra diferente irá gerar um intercepto e um coeficiente de inclinação diferentes. 

